{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro Large Language Model\n",
    "- A large language model is basically just two files, in a hypothetical directory. The files are parameters and a run file that contains the code that runs those parameters.\n",
    "- The parameters are the weights of the neural network that is the language model. Each parameter is stored as 2 bytes (because it is float 16 data type). So if a model (llama-2-70b) has 70 billion parameters, the file will be  about 140 GB in size.\n",
    "- We also need something that runs the neural network. This piece of code is implemented in the run file. This file can be in any language (C, python ..).\n",
    "- The C file contains only ~500 lines of code with no other dependencies to implement the neural network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Training\n",
    "- LLM Inference means just running the model on you local system.\n",
    "- Think of it like compressing the internet. \n",
    "- For the training data you crawl the internet websites and collect a large chunk of data.\n",
    "- After collecting large amount of data, get a huge cluster of GPUs ~6000 GPUs and run it for 12 days to get the parameters for llama2-70b\n",
    "- What a neural network does is it predicts the next word. this task of next work prediction forces a neural network to learn a lot about the world. And all this knowledge gets compressed into the parameters of an LLM."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
